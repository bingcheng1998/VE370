<img src="https://ws2.sinaimg.cn/large/006tNbRwly1fxhtku0vanj30qo050afg.jpg" width=320 />





# Artificial Neural Networks

## <center>VE370 Project 3</center>

























<center>Bingcheng HU</center>
<center>516021910219</center>
<center>December 7, 2018</center>

<div STYLE="page-break-after: always;"></div>
## <center>Contents</center>
[TOC]
<div STYLE="page-break-after: always;"></div>

## <center>Abstract</center>

**Key Words:** Convolutional Neural Network, Deep Learning, Artificial neural network (ANN)



## 1 Introduction￼￼



The Alpha Go, which combines CNNs and reinforcement learning, has already achieved a great success [Liu]. Compared with the supervised learning approaches, the unsupervised, semi-supervised and reinforcement-learning approaches, capable of overcoming the computational limitations, deserve further investigation.

## 2 Background 

### 2.1 Biological model and mathematical model of the neuron

<table rules=none frame=void><tr>
    <td><center> </center><img src=https://ws3.sinaimg.cn/large/006tNbRwly1fxsbcynp4xj30fm091wew.jpg border=0><center>(a)</center></td>
<td><img src=https://ws1.sinaimg.cn/large/006tNbRwly1fxrlxeg9g3j30fb0aqdgd.jpg border=0><center>
    (b)</center></td>
</tr></table>

<center>Fig. 1. Biological model of neuron(a) and architecture of Neural Network with hidden layers(b) [Yadav]</center>





### 2.2 Architectures: Restricted Boltzmann Machine

RBM was originally a concept proposed by Smolensky and has become prominent since Hinton published his work [59] in 2006. RBM consists of variants of Boltzmann machines (BMs). BM can be interpreted as NNs with a two-way connected stochastic processing unit [Liu].

It should be noted that the training process will be more efficient when using a gradient based contrast divergence (CD) algorithm. The CD algorithm for RBM training was developed by Hinton [56]. Algorithm 1 gives the process of the k-step CD algorithm [Liu].![](https://ws3.sinaimg.cn/large/006tNbRwly1fxse4jc3gvj31n20ridlk.jpg)

Assuming that the difference between the model and the target distribution is small, we can use the samples generated by the Gibbs chain to approximate the negative gradient. Ideally, as the chain length increases, its contribution to the probability decreases and tends to zero [12]. However, in [147] we can see that the estimate of the gradient does not represent the gradient itself. In addition, most CD components and corresponding log likelihood gradients have the same sign [45]. Therefore, a more practical algorithm called persistent contrast divergence is proposed in [115]. In this approach, the author suggests tracking the state of the persistent chain instead of searching for the initial value of the Gibbs Markov chain on a given data vector. After updating each weight, update the state of hidden and visible cells in the persistent chain. In this way, even a small learning rate does not create too much difference between the updated and persistent chain states, while at the same time leading to more accurate estimates.

### 2.3 Deep Learning Architectures: Deep Belief Network

As mentioned in the previous section, hidden and visible variables are not independent of each other [165]. To explore the dependencies between these variables, in 2006, Hinton built a DBN by stacking a set of RBMs. Specifically, the DBN consists of multiple layers of random and latent variables and can be considered a special form of the Bayesian probability generation model. DBNs are more efficient than artificial neural networks, especially when applied to problems with unlabeled data.![](https://ws3.sinaimg.cn/large/006tNbRwly1fxse92poyij31n00igaf0.jpg)

The visible layer of the lowest RBM is first trained, with $h(0)$ as input. The values in the visible layer are then imported into the hidden layer, where the activation probability $P(h | \mu)$ of the hidden variable is calculated. The representation obtained in the previous RBM will be used as training data for the next RBM, and the training process continues until all layers are traversed. Since the approximation of the likelihood function is only required in one step in the algorithm, the training time has been significantly reduced. Bad configuration problems that typically occur in deep networks can also be overcome during pre-training. This pre-training algorithm is also called a greedy layer-by-layer unsupervised training algorithm. For the sake of clarity, we provide its implementation in Algorithm 2 [Liu].

### 2.4 Deep Learning Architectures: Deep Convolutional Neural Networks

In a TDNN, the weights are shared in a temporal dimension, which leads to reduction in computation. 

The concept of CNNs is inspired by time-delay neural networks (TDNN). In CNNs, the convolution has replaced the general matrix multiplication in standard NNs. In this way, the number of weights is decreased, thereby reducing the complexity of the network. Furthermore, the images, as raw inputs, can be directly imported to the network, thus avoiding the feature extraction procedure in the standard learning algorithms. It should be noted that CNNs are the first truly successful deep learning architecture due to the successful training of the hierarchical layers. The CNN topology leverages spatial relationships so as to reduce the number of parameters in the network, and the performance is therefore improved using the standard backpropagation algorithms. Another advantage of the CNN model is that it requires minimal pre-processing.

![](https://ws4.sinaimg.cn/large/006tNbRwly1fxrl8co4ecj31920dujtz.jpg)

<center>Fig. 2. Schematic structure of CNNs [Liu]</center>


![](https://ws2.sinaimg.cn/large/006tNbRwly1fxrlakbf1oj318e0j8dhg.jpg)

<center>Fig. 3. Conceptual structure of CNNs Convolution [Liu]</center>

 Fewer parameters are required for CNN as compared to other traditional NN algorithms, which leads to reduction in memory and improvement in efficiency. The components of a standard CNN layer are shown in Figure 2, and a conceptual schematic diagram of a standard CNN is shown in Figure 3.

#### Convolution and sub-sampling

As shown in Figure 3, a CNN is a multi-layer neural network that consists of two different types of layers, i.e., convolution layers (c-layers) and sub-sampling layers (s-layers) [30], [74], [86]. C-layers and s-layers are connected alternately and form the middle part of the network. As Figure 4 shows, the input image is convolved with trainable filters at all possible offsets in order to produce feature maps in the first c-layer. A layer of connection weights are included in each filter. Normally, four pixels in the feature map form a group. Passed through a sigmoid function, these pixels produce additional feature maps in the first s-layer. This procedure carries on and we can thus obtain the feature maps in the following c-layers and s-layers. Finally, the values of these pixels are rasterized and displayed in a single vector as the input of the network [3].
Generally, c-layers are used to extract features when the input of each neuron is linked to the local receptive field of the previous layer. Once all the local features are extracted, the position relationship between them can be figured out. An s-layer is essentially a layer for feature mapping. These feature mapping layers share the weights and form a plane. Additionally, to achieve scale invariance, the sigmoid function is selected as the activation function due to its slight influence on the function kernel. It should also be noted that, the filters in this model are used to connect a series of overlapping receptive fields and transform the 2-D image batch input into a single unit in the output.

#### Pooling

As mentioned in [17], pooling is used to obtain invariance in image transformations. This process will lead to better robustness against noise. It is pointed out that the performance of various pooling methods depends on several factors, such as the resolution at which low-level features are extracted and the links between sample cardinalities. In 2011, Boureau [16] found that even if features are widely dissimilar, it is possible to pool them together as long as their locations are close. Furthermore, it is found that better performance can be delivered by performing clustering ahead of the pooling stage. In [78], it is shown that better pooling performance can be achieved by learning receptive fields more adaptively. Specifically, utilizing the concept of over-completeness, an efficient learning algorithm is proposed to accelerate the training process based on incremental feature selection.
More recently, Sermanet et al. [138] proposed a novel pooling method called Lp pooling and obtained high accuracy on the SVHN dataset. Lp pooling is a biological model inspired by complex cells. In 2013, Zeiler and Fergus [171] proposed a stochastic pooling method to regularize large CNNs which is equivalent to introduce a stochastic pooling procedure in each convolutional layer. According to a multinomial distribution, the activation is randomly selected in each pooling region. Moreover, since the selections in higher layers are independent of those in the lower ones, stochastic pooling is used to compute the deformations in a multi-layer model.


## 3 Applications

With rapid development of computation techniques, the GPU-accelerated computing techniques have been exploited to train CNNs more efficiently. Nowadays, CNNs have already been successfully applied to handwriting recognition, face detection, behavior recognition, speech recognition, recommender systems, image classification, ect.

### 3.1 Speech Recognition

During the past few decades, machine learning algorithms have been widely used in areas such as automatic speech recognition (ASR) and acoustic modeling [76], [116], [118], [126]. The ASR can be regarded as a standard classification problem which identifies word sequences from feature sequences or speech waveforms. The standard architecture of an ASR system is given in Figure 4.

![](https://ws2.sinaimg.cn/large/006tNbRwly1fxrlbnzdlcj31as0go40w.jpg)

<center>Fig. 4. Speech recognition system architecture [Liu]</center>

Early applications of the deep learning techniques consist of large vocabulary continuous speech recognition (LVCSR) [28] and phone recognition [117]–[119]. In 2015, the DNNs have been employed in automatic language identification (LID). Experiments have been carried out on short test utterance [49] from two datasets: the Google 5 million utterances LID and the public NIST Language Recognition Evaluation dataset. More recently, a multi-task learning (MTL) method is proposed to improve the low-resource ASR using DNNs with no requirement on additional language resources [20]. 

### 3.2 Computer Vision and Pattern Recognition



![](https://ws3.sinaimg.cn/large/006tNbRwly1fxrlh8uv96j31ds0ha4j7.jpg)

<center>Fig. 5. Computer Vision and Pattern Recognition Usage [Kendall]</center>



### 3.3 Playing board games

Tic-tac-toe (also known as noughts and crosses or Xs and 0s) is a pen and paper game for two players $\times$ and $\circ$, which alternately mark space in a $3 \times 3$ grid. A player who successfully placed three markers on a horizontal, vertical or diagonal line wins the game [Kaplan].

In this experiment, each square in the large well was divided into 9 small squares and one small well. Initially, the first player selects 1 large square and places $\times$ or $\circ$ in one of the 9 small squares of the large square. Then the next player should choose a large square whose relative position is the same as the relative positive value of the small square of the well. The game is like this rule [Chen].

![](https://ws2.sinaimg.cn/large/006tNbRwly1fxrlkxfv95j314y0cswl3.jpg)

<center>Fig. 6. Blue player wins a Tic-Tac-Toe in Big Well, and the whole game [Chen]</center>

As shown in Figure 6, by entering the data set generated by the Monte Carlo tree search, the algorithm can automatically improve its board game skills. Because the architecture of the neural network is simple, the training efficiency is not very good. If more versions are developed, a better architecture can lead to better performance [Chen].

### 3.4 Predicting game outcomes in Dota 2

Dota 2 is an online strategy game, played in a `five versus five` format. Its multitude of selectable characters, each with a unique set of abilities and spells, causes every new match to be different from the last and picking the right characters can ultimately decide whether a team wins or loses a game [Widin].

For the ANN to be useful it needs to be able to learn. It does this with a technique
called backpropagation that allows it to quickly update the weights between the neurons in the ANN. [8] The backpropagation algorithm utilizes the gradient ∂E of the loss function E in ∂w respect to any weight w. [4] After each epoch, the weights are modified so as to minimize the mean-squared error between the neural network’s prediction and the actual target value.

Every ANN was trained for 5000 epochs using the training data. After each step of 1000 epochs we noted the current Mean Squared Error for the training set and tested the ANN. And the testing result is shown as Figure 7.

<center>
<img src="https://ws2.sinaimg.cn/large/006tNbRwly1fxsi1uvi8jj30fm08vq3t.jpg" width="70%" />
</center>

<center>Fig. 7. Testing results after training for different models.[Widin]</center>

The results show that all models have a prediction rate above 50%. The average accuracy for the models ranges between 53.44% and 59.54%.

##  4 Topics for future research

### 4.1 Design a depth model to learn from less training data

When only a limited amount of training data is available, a more powerful model is needed to achieve enhanced learning capabilities. Therefore, it is important to consider how to design a deep model to learn from less training data, especially for speech and visual recognition systems.

### 4.2 Optimization algorithms for adjusting network parameters

The method of adjusting parameters in machine learning algorithms is an emerging topic in computer science. In the DNN, a large number of parameters need to be adjusted. In addition, as the number of hidden nodes increases, the algorithm is more likely to fall into local optimum. [N. Zeng]

### 4.3 Unsupervised, semi-supervised and intensive learning methods applied to DNNs in complex systems

Deep learning techniques have not achieved satisfactory results in NLP. With the development of deep unsupervised learning and deep reinforcement learning, we have more choices to train DNNs in complex systems.

### 4.4 Implement deep learning algorithms on mobile devices

It should be noted that deep learning methods, especially CNN, usually require a large computational burden. Recently, the idea of deep learning chips has emerged and has attracted a lot of research attention. Researchers at the Massachusetts Institute of Technology have proposed a chip for neural network implementation [Liu].

### 4.5 Deep Neural Network Stability Analysis

Dynamic neural networks have been widely used to solve optimization problems and are used in many engineering applications. Today, the stability analysis of deep neural networks has become a hot research topic because it brings many benefits to industry [J. Hu].

### 4.6 Application of deep neural networks in nonlinear networked control systems

Neural networks have been widely used to control engineering and signal processing to approximate nonlinear systems. On the other hand, NCS has been extensively studied so far. It is natural to apply deep neural networks to approximate nonlinear NCS with complex dynamics for better control/filtering performance [H. Chen].

## 5 Discussion

## 6 Conclusion

<div STYLE="page-break-after: always;"></div>

## <center>References</center>

[1]  Kubat, M. (2015). Artificial neural networks. In *An Introduction to Machine Learning* (pp. 91-111). Springer, Cham.

[2]  Klerfors, D., & Huston, T. L. (1998). Artificial neural networks. *St. Louis University, St. Louis, Mo*.

[Liu] Liu, W., Wang, Z., Liu, X., Zeng, N., Liu, Y., & Alsaadi, F. E. (2017). A survey of deep neural network architectures and their applications. *Neurocomputing*, *234*, 11-26.

[Widin]  Widin, V., & Adler, J. (2017). On using Artificial Neural Network models to predict game outcomes in Dota 2.

[Yadav]  Yadav, A., & Sahu, K. (2017). WIND FORECASTING USING ARTIFICIAL NEURAL NETWORKS: A SURVEY AND TAXONOMY. *International Journal of Research In Science & Engineering*, *3*.

[Chen] Chen, W. (2017). Using Neural Network and Monte-Carlo Tree Search to Play the Game TEN.

[Kendall] Kendall, A., & Gal, Y. (2017). What uncertainties do we need in bayesian deep learning for computer vision?. In Advances in neural information processing systems (pp. 5574-5584).

 [N. Zeng] Zeng, N., Wang, Z., Zhang, H., & Alsaadi, F. E. (2016). A novel switching delayed PSO algorithm for estimating unknown parameters of lateral flow immunoassay. Cognitive Computation, 8(2), 143-152.

[J. Hu] Hu, J., Chen, D., & Du, J. (2014). State estimation for a class of discrete nonlinear systems with randomly occurring uncertainties and distributed sensor delays. International Journal of General Systems, 43(3-4), 387-401.

[H. Chen] Chen, H., Liang, J., & Wang, Z. (2016). Pinning controllability of autonomous Boolean control networks. Science China Information Sciences, 59(7), 070107.

[Kaplan] Kaplan, E. (1999). U.S. Patent No. 5,927,714. Washington, DC: U.S. Patent and Trademark Office.